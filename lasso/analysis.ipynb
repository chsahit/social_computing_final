{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from langdetect import detect_langs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Bag-of-word Vectors for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify all words that either identify \"male\" or \"female\"\n",
    "def get_gendered_words():\n",
    "    vocab10K=pd.read_csv(\"../vocab10K.csv\")\n",
    "    male = vocab10K.loc[vocab10K['female'] == 0,:]\n",
    "    male = male['word'].tolist()\n",
    "    female = vocab10K.loc[vocab10K['female'] == 1,:]\n",
    "    female = female['word'].tolist()\n",
    "    return male, female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes either \"twitter\" or \"reddit\" as a parameter.\n",
    "# Goes through each post and identifies the one using only male words as male\n",
    "# and female words as female.  \n",
    "# returns a list of unambiguous posts and a vector of male (0) and female (1) words\n",
    "def identify_gendered_posts(male, female, platform = \"twitter\"):\n",
    "    path = \"../custom_data/all_tweets.csv\"\n",
    "    if platform == \"reddit\":\n",
    "        path = \"../custom_data/economics_posts.csv\"\n",
    "    data = pd.read_csv(path)\n",
    "    vocab = pd.read_csv(\"../vocab10K.csv\")\n",
    "    num_entries = data.shape[0]\n",
    "    y = list()\n",
    "    male_set = set(male)\n",
    "    female_set = set(female)\n",
    "    nFemale = 0\n",
    "    nMale = 0\n",
    "    unambiguous_posts = list()\n",
    "    for i in range(num_entries):\n",
    "        if i % 1000 == 0:\n",
    "            print(str(i) + \" posts scanned\")\n",
    "        post = str(data['text'][i]).lower()\n",
    "        lang = \"en\"\n",
    "        try:\n",
    "            lang = str(detect_langs(post))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        if \"en\" not in lang:\n",
    "            continue\n",
    "        i_male, i_female = 0, 0\n",
    "        for word in post.split():\n",
    "            if word in male_set:\n",
    "                i_male += 1\n",
    "            elif word in female_set:\n",
    "                i_female += 1\n",
    "        if i_male > 0 and i_female == 0:\n",
    "            y.append(0)\n",
    "            nMale += 1\n",
    "            unambiguous_posts.append(post)\n",
    "        if i_female > 0 and i_male == 0:\n",
    "            y.append(1)\n",
    "            nFemale += 1\n",
    "            unambiguous_posts.append(post)\n",
    "    print(\"nMale: \", nMale)\n",
    "    print(\"nFemale: \", nFemale)\n",
    "    return unambiguous_posts, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts a matrix of unambiguous posts to a bag of words. \n",
    "# Saves the bow and labels to a text file corresponding to the platform\n",
    "def build_bow(unambiguous_posts, y, platform = \"twitter\"):\n",
    "    vocab = pd.read_csv(\"../vocab10K.csv\")\n",
    "    num_entries = len(unambiguous_posts)\n",
    "    X = np.zeros((num_entries, 10000))\n",
    "    vocab_list = vocab['word'].tolist()\n",
    "    vocab_set = set(vocab_list)\n",
    "    for i in range(num_entries):\n",
    "        if i % 100 == 0:\n",
    "            print(str(i) + \" rows populated\")\n",
    "        post = unambiguous_posts[i].split()\n",
    "        for word in post:\n",
    "            if word in vocab_set:\n",
    "                X[i, vocab_list.index(word)] = 1\n",
    "    if platform == \"twitter\":\n",
    "        np.save(\"twitter_data\", X)\n",
    "        np.save(\"twitter_labels\", y)\n",
    "    elif platform == \"reddit\":\n",
    "        np.save(\"reddit_data\", X)\n",
    "        np.save(\"reddit_labels\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a LASSO model on the BOW and labels and analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train lasso function defined in lasso.py\n",
    "from lasso import train_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the coefficients learned from lasso and extracts which words were the most predictive\n",
    "def get_top_words_from_model(platform = \"twitter\"):\n",
    "    coeffs = np.loadtxt(\"../coef_twitter.txt\")\n",
    "    if platform == \"reddit\":\n",
    "        coeffs = np.loadtxt(\"../coef_reddit.txt\")\n",
    "    women_words = coeffs.argsort()[-15:][::-1]\n",
    "    men_words = coeffs.argsort()[:15]\n",
    "    # look at which words correspond to the most important coeff\n",
    "    word_map = np.loadtxt(\"../i_keep_columns.txt\")\n",
    "    women_indices = word_map[women_words] + 1\n",
    "    men_indices = word_map[men_words] + 1\n",
    "    print(\"women: \")\n",
    "    print(vocablist.loc[vocablist['index'].isin(women_indices)]['word'])\n",
    "    print(\"men: \")\n",
    "    print(vocablist.loc[vocablist['index'].isin(men_indices)]['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identifying gendered words\n",
      "identifying gendered posts\n",
      "0 posts scanned\n",
      "1000 posts scanned\n",
      "2000 posts scanned\n",
      "3000 posts scanned\n",
      "4000 posts scanned\n",
      "5000 posts scanned\n",
      "6000 posts scanned\n",
      "7000 posts scanned\n",
      "8000 posts scanned\n",
      "9000 posts scanned\n",
      "10000 posts scanned\n",
      "11000 posts scanned\n",
      "12000 posts scanned\n",
      "13000 posts scanned\n",
      "14000 posts scanned\n",
      "15000 posts scanned\n",
      "nMale:  996\n",
      "nFemale:  600\n",
      "building bag of words\n",
      "0 rows populated\n",
      "100 rows populated\n",
      "200 rows populated\n",
      "300 rows populated\n",
      "400 rows populated\n",
      "500 rows populated\n",
      "600 rows populated\n",
      "700 rows populated\n",
      "800 rows populated\n",
      "900 rows populated\n",
      "1000 rows populated\n",
      "1100 rows populated\n",
      "1200 rows populated\n",
      "1300 rows populated\n",
      "1400 rows populated\n",
      "1500 rows populated\n",
      "training model\n",
      "full X:  (1596, 10000)\n",
      "train:  (1436, 9540)\n",
      "train:  (1436,)\n",
      "test:  (160, 9540)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.5s remaining:    2.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.155441458743784\n",
      "running analysis\n",
      "women: \n",
      "282          higher\n",
      "436       including\n",
      "463           macro\n",
      "480           black\n",
      "483          please\n",
      "486         support\n",
      "580         talking\n",
      "667           share\n",
      "877      colleagues\n",
      "2040    suggestions\n",
      "2245      colleague\n",
      "2579            aea\n",
      "3512     bargaining\n",
      "6426       murdered\n",
      "7286      discusses\n",
      "Name: word, dtype: object\n",
      "men: \n",
      "153             too\n",
      "207          theory\n",
      "221           point\n",
      "276             pay\n",
      "383             yet\n",
      "475       professor\n",
      "515         harvard\n",
      "532            mind\n",
      "966          modern\n",
      "979           death\n",
      "1196          types\n",
      "1368          views\n",
      "2142          curve\n",
      "4223    forthcoming\n",
      "5927    fascinating\n",
      "Name: word, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# running everything in unison\n",
    "platform = \"twitter\"\n",
    "print(\"identifying gendered words\")\n",
    "male, female = get_gendered_words()\n",
    "print(\"identifying gendered posts\")\n",
    "unambiguous_posts, y = identify_gendered_posts(male, female,  platform = platform)\n",
    "print(\"building bag of words\")\n",
    "build_bow(unambiguous_posts, y, platform = platform)\n",
    "print(\"training model\")\n",
    "train_lasso(platform = platform)\n",
    "print(\"running analysis\")\n",
    "get_top_words_from_model(platform = platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464\n"
     ]
    }
   ],
   "source": [
    "# count instances of word \"Yellen\" (case-insensitive)\n",
    "import pandas as pd\n",
    "vocab = pd.read_csv(\"../gendered_posts.csv\")\n",
    "list_of_posts = vocab['raw_post'].tolist()\n",
    "y_count = 0\n",
    "for post in list_of_posts:\n",
    "    p = post.lower()\n",
    "    if \"yellen\" in p:\n",
    "        y_count += 1\n",
    "print(y_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.88317862 2.20474637 1.89043858 1.88126299 1.84982683 1.77917599\n",
      " 1.74436485 1.67609757 1.64098876 1.55399143]\n",
      "      index        word  female  i_pronoun  exclude      coef        ME  \\\n",
      "1106   1107         hot     NaN        NaN      NaN  1.849972  0.271002   \n",
      "1530   1531   beautiful     NaN        NaN      NaN  1.637034  0.239809   \n",
      "1696   1697  attractive     NaN        NaN      NaN  1.672941  0.245069   \n",
      "3298   3299       marry     NaN        NaN      NaN  1.879690  0.275356   \n",
      "4135   4136    pregnant     NaN        NaN      NaN  2.203002  0.322718   \n",
      "5067   5068   pregnancy     NaN        NaN      NaN  1.734884  0.254143   \n",
      "7146   7147    marrying     NaN        NaN      NaN  1.775510  0.260094   \n",
      "7889   7890      breast     NaN        NaN      NaN  1.546361  0.226526   \n",
      "8914   8915      hotter     NaN        NaN      NaN  2.882882  0.422313   \n",
      "9137   9138        plow     NaN        NaN      NaN  1.892775  0.277273   \n",
      "\n",
      "      nFemale  nMale  coef_pronoun  ME_pronoun  nFemale_pronoun  \\\n",
      "1106     3613   1053      1.220669    0.197421             1309   \n",
      "1530     1419    610      0.925590    0.149697              524   \n",
      "1696     1578    417      1.120419    0.181208              547   \n",
      "3298     1287    258      1.281486    0.207257              557   \n",
      "4135      564    120      1.597201    0.258318              270   \n",
      "5067      202     61      1.807278    0.292295              106   \n",
      "7146      262     49      1.222155    0.197662              117   \n",
      "7889      134     48      1.361359    0.220175               62   \n",
      "8914      307     31      1.789012    0.289340              120   \n",
      "9137      274     83      1.354111    0.219003              146   \n",
      "\n",
      "      nMale_pronoun  linear_coef  \n",
      "1106            658     0.215589  \n",
      "1530            346     0.136367  \n",
      "1696            246     0.189988  \n",
      "3298            191     0.225843  \n",
      "4135             98     0.239631  \n",
      "5067             27     0.172747  \n",
      "7146             34     0.058259  \n",
      "7889             30     0.055059  \n",
      "8914             31     0.203085  \n",
      "9137             60     0.244884  \n"
     ]
    }
   ],
   "source": [
    "# Reproduction of Wu's results:\n",
    "# (1) get most important coeffs from lasso-logit-full\n",
    "coeffs = np.loadtxt(\"../coef_lasso_logit_full.txt\")\n",
    "top_words = coeffs.argsort()[-10:][::-1]\n",
    "print(coeffs[top_words])\n",
    "# (2) look at which words correspond to the most important coeff\n",
    "vocablist = pd.read_csv(\"../vocab10K.csv\")\n",
    "word_map = np.loadtxt(\"../i_keep_columns.txt\")\n",
    "vocab_indices = word_map[top_words] + 1\n",
    "print(vocablist.loc[vocablist['index'].isin(vocab_indices)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
