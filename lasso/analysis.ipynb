{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from langdetect import detect_langs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Bag-of-word Vectors for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify all words that either identify \"male\" or \"female\"\n",
    "def get_gendered_words():\n",
    "    vocab10K=pd.read_csv(\"../vocab10K.csv\")\n",
    "    male = vocab10K.loc[vocab10K['female'] == 0,:]\n",
    "    male = male['word'].tolist()\n",
    "    female = vocab10K.loc[vocab10K['female'] == 1,:]\n",
    "    female = female['word'].tolist()\n",
    "    return male, female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes either \"twitter\" or \"reddit\" as a parameter.\n",
    "# Goes through each post and identifies the one using only male words as male\n",
    "# and female words as female.  \n",
    "# returns a list of unambiguous posts and a vector of male (0) and female (1) words\n",
    "def identify_gendered_posts(male, female, platform = \"twitter\"):\n",
    "    path = \"../custom_data/all_tweets.csv\"\n",
    "    if platform == \"reddit\":\n",
    "        path = \"../custom_data/economics_posts.csv\"\n",
    "    data = pd.read_csv(path)\n",
    "    vocab = pd.read_csv(\"../vocab10K.csv\")\n",
    "    num_entries = data.shape[0]\n",
    "    y = list()\n",
    "    male_set = set(male)\n",
    "    female_set = set(female)\n",
    "    nFemale = 0\n",
    "    nMale = 0\n",
    "    unambiguous_posts = list()\n",
    "    for i in range(num_entries):\n",
    "        if i % 1000 == 0:\n",
    "            print(str(i) + \" posts scanned\")\n",
    "        post = str(data['text'][i]).lower()\n",
    "        lang = \"en\"\n",
    "        try:\n",
    "            lang = str(detect_langs(post))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        if \"en\" not in lang:\n",
    "            continue\n",
    "        i_male, i_female = 0, 0\n",
    "        for word in post.split():\n",
    "            if word in male_set:\n",
    "                i_male += 1\n",
    "            elif word in female_set:\n",
    "                i_female += 1\n",
    "        if i_male > 0 and i_female == 0:\n",
    "            y.append(0)\n",
    "            nMale += 1\n",
    "            unambiguous_posts.append(post)\n",
    "        if i_female > 0 and i_male == 0:\n",
    "            y.append(1)\n",
    "            nFemale += 1\n",
    "            unambiguous_posts.append(post)\n",
    "    print(\"nMale: \", nMale)\n",
    "    print(\"nFemale: \", nFemale)\n",
    "    return unambiguous_posts, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts a matrix of unambiguous posts to a bag of words. \n",
    "# Saves the bow and labels to a text file corresponding to the platform\n",
    "def build_bow(unambiguous_posts, y, platform = \"twitter\"):\n",
    "    vocab = pd.read_csv(\"../vocab10K.csv\")\n",
    "    num_entries = len(unambiguous_posts)\n",
    "    X = np.zeros((num_entries, 10000))\n",
    "    vocab_list = vocab['word'].tolist()\n",
    "    vocab_set = set(vocab_list)\n",
    "    for i in range(num_entries):\n",
    "        if i % 100 == 0:\n",
    "            print(str(i) + \" rows populated\")\n",
    "        post = unambiguous_posts[i].split()\n",
    "        for word in post:\n",
    "            if word in vocab_set:\n",
    "                X[i, vocab_list.index(word)] = 1\n",
    "    if platform == \"twitter\":\n",
    "        np.save(\"twitter_data\", X)\n",
    "        np.save(\"twitter_labels\", y)\n",
    "    elif platform == \"reddit\":\n",
    "        np.save(\"reddit_data\", X)\n",
    "        np.save(\"reddit_labels\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a LASSO model on the BOW and labels and analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train lasso function defined in lasso.py\n",
    "from lasso import train_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the coefficients learned from lasso and extracts which words were the most predictive\n",
    "def get_top_words_from_model(platform = \"twitter\"):\n",
    "    vocablist = pd.read_csv(\"../vocab10K.csv\")\n",
    "    coeffs = np.loadtxt(\"../coef_twitter.txt\")\n",
    "    if platform == \"reddit\":\n",
    "        coeffs = np.loadtxt(\"../coef_reddit.txt\")\n",
    "    women_words = coeffs.argsort()[-15:][::-1]\n",
    "    men_words = coeffs.argsort()[:15]\n",
    "    # look at which words correspond to the most important coeff\n",
    "    word_map = np.loadtxt(\"../i_keep_columns.txt\")\n",
    "    women_indices = word_map[women_words] + 1\n",
    "    men_indices = word_map[men_words] + 1\n",
    "    print(\"women: \")\n",
    "    print(vocablist.loc[vocablist['index'].isin(women_indices)]['word'])\n",
    "    print(\"men: \")\n",
    "    print(vocablist.loc[vocablist['index'].isin(men_indices)]['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identifying gendered words\n",
      "identifying gendered posts\n",
      "0 posts scanned\n",
      "1000 posts scanned\n",
      "2000 posts scanned\n",
      "3000 posts scanned\n",
      "4000 posts scanned\n",
      "5000 posts scanned\n",
      "6000 posts scanned\n",
      "7000 posts scanned\n",
      "8000 posts scanned\n",
      "9000 posts scanned\n",
      "10000 posts scanned\n",
      "11000 posts scanned\n",
      "12000 posts scanned\n",
      "13000 posts scanned\n",
      "14000 posts scanned\n",
      "15000 posts scanned\n",
      "nMale:  996\n",
      "nFemale:  600\n",
      "building bag of words\n",
      "0 rows populated\n",
      "100 rows populated\n",
      "200 rows populated\n",
      "300 rows populated\n",
      "400 rows populated\n",
      "500 rows populated\n",
      "600 rows populated\n",
      "700 rows populated\n",
      "800 rows populated\n",
      "900 rows populated\n",
      "1000 rows populated\n",
      "1100 rows populated\n",
      "1200 rows populated\n",
      "1300 rows populated\n",
      "1400 rows populated\n",
      "1500 rows populated\n",
      "training model\n",
      "full X:  (1596, 10000)\n",
      "train:  (1436, 9540)\n",
      "train:  (1436,)\n",
      "test:  (160, 9540)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.4s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.264828021450747\n",
      "running analysis\n",
      "women: \n",
      "282          higher\n",
      "436       including\n",
      "463           macro\n",
      "480           black\n",
      "483          please\n",
      "486         support\n",
      "580         talking\n",
      "667           share\n",
      "877      colleagues\n",
      "2040    suggestions\n",
      "2245      colleague\n",
      "2579            aea\n",
      "3512     bargaining\n",
      "6426       murdered\n",
      "7286      discusses\n",
      "Name: word, dtype: object\n",
      "men: \n",
      "153             too\n",
      "207          theory\n",
      "221           point\n",
      "276             pay\n",
      "383             yet\n",
      "475       professor\n",
      "515         harvard\n",
      "532            mind\n",
      "966          modern\n",
      "979           death\n",
      "1196          types\n",
      "1368          views\n",
      "2142          curve\n",
      "4223    forthcoming\n",
      "5927    fascinating\n",
      "Name: word, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# running everything in unison\n",
    "platform = \"twitter\"\n",
    "print(\"identifying gendered words\")\n",
    "male, female = get_gendered_words()\n",
    "print(\"identifying gendered posts\")\n",
    "unambiguous_posts, y = identify_gendered_posts(male, female,  platform = platform)\n",
    "print(\"building bag of words\")\n",
    "build_bow(unambiguous_posts, y, platform = platform)\n",
    "print(\"training model\")\n",
    "train_lasso(platform = platform)\n",
    "print(\"running analysis\")\n",
    "get_top_words_from_model(platform = platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count instances of word \"Yellen\" (case-insensitive)\n",
    "import pandas as pd\n",
    "vocab = pd.read_csv(\"../gendered_posts.csv\")\n",
    "list_of_posts = vocab['raw_post'].tolist()\n",
    "y_count = 0\n",
    "for post in list_of_posts:\n",
    "    p = post.lower()\n",
    "    if \"yellen\" in p:\n",
    "        y_count += 1\n",
    "print(y_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduction of Wu's results:\n",
    "# (1) get most important coeffs from lasso-logit-full\n",
    "coeffs = np.loadtxt(\"../coef_lasso_logit_full.txt\")\n",
    "top_words = coeffs.argsort()[-10:][::-1]\n",
    "print(coeffs[top_words])\n",
    "# (2) look at which words correspond to the most important coeff\n",
    "vocablist = pd.read_csv(\"../vocab10K.csv\")\n",
    "word_map = np.loadtxt(\"../i_keep_columns.txt\")\n",
    "vocab_indices = word_map[top_words] + 1\n",
    "print(vocablist.loc[vocablist['index'].isin(vocab_indices)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
