{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from langdetect import detect_langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.88317862 2.20474637 1.89043858 1.88126299 1.84982683 1.77917599\n",
      " 1.74436485 1.67609757 1.64098876 1.55399143]\n"
     ]
    }
   ],
   "source": [
    "# get most important coeffs from lasso-logit-full\n",
    "coeffs = np.loadtxt(\"../coef_lasso_logit_full.txt\")\n",
    "top_words = coeffs.argsort()[-10:][::-1]\n",
    "print(coeffs[top_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      index        word  female  i_pronoun  exclude      coef        ME  \\\n",
      "1106   1107         hot     NaN        NaN      NaN  1.849972  0.271002   \n",
      "1530   1531   beautiful     NaN        NaN      NaN  1.637034  0.239809   \n",
      "1696   1697  attractive     NaN        NaN      NaN  1.672941  0.245069   \n",
      "3298   3299       marry     NaN        NaN      NaN  1.879690  0.275356   \n",
      "4135   4136    pregnant     NaN        NaN      NaN  2.203002  0.322718   \n",
      "5067   5068   pregnancy     NaN        NaN      NaN  1.734884  0.254143   \n",
      "7146   7147    marrying     NaN        NaN      NaN  1.775510  0.260094   \n",
      "7889   7890      breast     NaN        NaN      NaN  1.546361  0.226526   \n",
      "8914   8915      hotter     NaN        NaN      NaN  2.882882  0.422313   \n",
      "9137   9138        plow     NaN        NaN      NaN  1.892775  0.277273   \n",
      "\n",
      "      nFemale  nMale  coef_pronoun  ME_pronoun  nFemale_pronoun  \\\n",
      "1106     3613   1053      1.220669    0.197421             1309   \n",
      "1530     1419    610      0.925590    0.149697              524   \n",
      "1696     1578    417      1.120419    0.181208              547   \n",
      "3298     1287    258      1.281486    0.207257              557   \n",
      "4135      564    120      1.597201    0.258318              270   \n",
      "5067      202     61      1.807278    0.292295              106   \n",
      "7146      262     49      1.222155    0.197662              117   \n",
      "7889      134     48      1.361359    0.220175               62   \n",
      "8914      307     31      1.789012    0.289340              120   \n",
      "9137      274     83      1.354111    0.219003              146   \n",
      "\n",
      "      nMale_pronoun  linear_coef  \n",
      "1106            658     0.215589  \n",
      "1530            346     0.136367  \n",
      "1696            246     0.189988  \n",
      "3298            191     0.225843  \n",
      "4135             98     0.239631  \n",
      "5067             27     0.172747  \n",
      "7146             34     0.058259  \n",
      "7889             30     0.055059  \n",
      "8914             31     0.203085  \n",
      "9137             60     0.244884  \n"
     ]
    }
   ],
   "source": [
    "# look at which words correspond to the most important coeff\n",
    "vocablist = pd.read_csv(\"../vocab10K.csv\")\n",
    "word_map = np.loadtxt(\"../i_keep_columns.txt\")\n",
    "vocab_indices = word_map[top_words] + 1\n",
    "print(vocablist.loc[vocablist['index'].isin(vocab_indices)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER: dump all the gender coded words into a file\n",
    "vocab10K=pd.read_csv(\"../vocab10K.csv\")\n",
    "male = vocab10K.loc[vocab10K['female'] == 0,:]\n",
    "male = male['word'].tolist()\n",
    "female = vocab10K.loc[vocab10K['female'] == 1,:]\n",
    "female = female['word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "nMale:  2135\n",
      "nFemale:  299\n"
     ]
    }
   ],
   "source": [
    "# HELPER: label data\n",
    "#path = \"../custom_data/all_tweets.csv\"\n",
    "path = \"../custom_data/economics_posts.csv\"\n",
    "data = pd.read_csv(path)\n",
    "vocab = pd.read_csv(\"../vocab10K.csv\")\n",
    "num_entries = data.shape[0]\n",
    "y = list()\n",
    "male_set = set(male)\n",
    "female_set = set(female)\n",
    "nFemale = 0\n",
    "nMale = 0\n",
    "unambiguous_posts = list()\n",
    "for i in range(num_entries):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    post = str(data['text'][i]).lower()\n",
    "    lang = \"en\"\n",
    "    try:\n",
    "        lang = str(detect_langs(post))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    if \"en\" not in lang:\n",
    "        continue\n",
    "    i_male, i_female = 0, 0\n",
    "    for word in post.split():\n",
    "        if word in male_set:\n",
    "            i_male += 1\n",
    "        elif word in female_set:\n",
    "            i_female += 1\n",
    "    if i_male > 0 and i_female == 0:\n",
    "        y.append(0)\n",
    "        nMale += 1\n",
    "        unambiguous_posts.append(post)\n",
    "    if i_female > 0 and i_male == 0:\n",
    "        y.append(1)\n",
    "        nFemale += 1\n",
    "        unambiguous_posts.append(post)\n",
    "print(\"nMale: \", nMale)\n",
    "print(\"nFemale: \", nFemale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n"
     ]
    }
   ],
   "source": [
    "# Helper: make bow\n",
    "vocab = pd.read_csv(\"../vocab10K.csv\")\n",
    "num_entries = len(unambiguous_posts)\n",
    "X = np.zeros((num_entries, 10000))\n",
    "vocab_list = vocab['word'].tolist()\n",
    "vocab_set = set(vocab_list)\n",
    "for i in range(num_entries):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    post = unambiguous_posts[i].split()\n",
    "    for word in post:\n",
    "        if word in vocab_set:\n",
    "            X[i, vocab_list.index(word)] = 1\n",
    "\n",
    "np.save(\"reddit_data\", X)\n",
    "np.save(\"reddit_labels\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "women: \n",
      "6          and\n",
      "18          it\n",
      "53          my\n",
      "109        had\n",
      "113       want\n",
      "145         am\n",
      "200       both\n",
      "206       look\n",
      "281    student\n",
      "414       home\n",
      "462       kids\n",
      "529     living\n",
      "586      spend\n",
      "614       debt\n",
      "665       stay\n",
      "Name: word, dtype: object\n",
      "men: \n",
      "14         that\n",
      "32         this\n",
      "46         will\n",
      "72        which\n",
      "148         job\n",
      "160        same\n",
      "187        back\n",
      "203    economic\n",
      "238        give\n",
      "258         end\n",
      "286       using\n",
      "328       large\n",
      "367      system\n",
      "796     company\n",
      "841      worked\n",
      "Name: word, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# get most important coeffs from lasso-reddit\n",
    "coeffs = np.loadtxt(\"../coef_reddit.txt\")\n",
    "women_words = coeffs.argsort()[-15:][::-1]\n",
    "men_words = coeffs.argsort()[:15]\n",
    "# look at which words correspond to the most important coeff\n",
    "word_map = np.loadtxt(\"../i_keep_columns.txt\")\n",
    "women_indices = word_map[women_words] + 1\n",
    "men_indices = word_map[men_words] + 1\n",
    "print(\"women: \")\n",
    "print(vocablist.loc[vocablist['index'].isin(women_indices)]['word'])\n",
    "print(\"men: \")\n",
    "print(vocablist.loc[vocablist['index'].isin(men_indices)]['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464\n"
     ]
    }
   ],
   "source": [
    "# count instances of word \"Yellen\" (case-insensitive)\n",
    "import pandas as pd\n",
    "vocab = pd.read_csv(\"../gendered_posts.csv\")\n",
    "list_of_posts = vocab['raw_post'].tolist()\n",
    "y_count = 0\n",
    "for post in list_of_posts:\n",
    "    p = post.lower()\n",
    "    if \"yellen\" in p:\n",
    "        y_count += 1\n",
    "print(y_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
